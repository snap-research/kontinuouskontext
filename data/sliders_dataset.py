import torch 
import numpy as np
import torch.nn as nn 
import json 
from torch.utils.data import Dataset
from PIL import Image
import os 
import random 
import lpips
from torchvision import transforms


# This is the custom dataloader for the sliders datasset, it will load the input and edited image, prompt and the slider values for the edits. 
class SliderDataset(Dataset):
    def __init__(
        self,
        data_json_path, # path where the .json file will be present, that have the filenames and the edits 
        image_dataset_path, # path for the image dataset, that will have the stack of images with original and the interpolated images that need to be processed to obtain pairs 
        image_size: int = 512, # size of the input image on which we will be training 
        drop_text_prob: float = 0.1, # the text conditioning will be dropped with this probability 
        drop_slider_prob: float = 0.1, # the slider conditionining will be dropped with this probability 
        filter: str = "no-filter",
        kl_threshold: float = 0.15,
        return_pil_image: bool = False, # 0.15 # the threshold for the kl divergence between the sample lpips sequence and the uniform distribution 
    ):

        # Loading the paths for all the required components of the dataset 
        self.imgs_dir = image_dataset_path # this folder will have stack of images generated by the interpolation models and filtereed based on some criteria 
        # this metadata will have .json and the information for all the images stacked in the single .json file 
        self.meta_data_path = data_json_path
        # Loading the meta data directly from the .json that will be used for processing in the dataloader 
        self.meta_data = json.load(open(self.meta_data_path)) # loading the meta data from the json file 

        self.filter = filter
        print("------------------- using the filter to remove the samples: {} for the dataset -------------------".format(self.filter))
        self.kl_threshold = kl_threshold
        print("---------------- using KL based filtering with threshold: {} ----------------".format(self.kl_threshold)) 

        # filter meta data based on the the edit category, if the edit category is appearance 
        # self.meta_data = [item for item in self.meta_data if item['category'] == 'stylization']  

        print("--------- meta data loaded with n_images: {} ----------".format(len(self.meta_data))) 
        self.meta_data = self.filter_data(self.meta_data)
        print("------------------------------------- filtered meta data size: {} ------------------------------".format(len(self.meta_data))) 

        print("sample meta data: {}".format(self.meta_data[0])) 

        self.img_size = image_size
        self.drop_text_prob = drop_text_prob
        self.drop_slider_prob = drop_slider_prob
        self.return_pil_image = return_pil_image

        # defining the lpips function for computation of distance between samples. 
        loss_fn_vgg = lpips.LPIPS(net='vgg')
        self.loss_fn_vgg = loss_fn_vgg 
        self.loss_fn_vgg.eval()

        print("----- Using drop text with prob: {} -----".format(self.drop_text_prob))

        self.image_transforms = transforms.Compose(
                [
                    transforms.ToTensor(),
                    transforms.Normalize([0.5], [0.5]),
                ]
            )
        self.to_tensor = transforms.ToTensor()
        self.custom_instance_prompts = ["temp just to run the code"]

    def __len__(self):
        return len(self.meta_data)  

    # This function will filter out the samples that are not valid based on the given criteria and return the valid dataset with the filtered samples
    def filter_data(self, image_meta_data):
        # accumulating the samples where the metadata is valid and ignoring the other samples
        filtered_meta_data = []
        for image_meta_data_sample in image_meta_data: 
            # Extacting the image name and the edit instruction from the meta data to perform the processing 
            image_name = image_meta_data_sample['image_name'] # getting the image name 
            edit_instruction = image_meta_data_sample['edit_instruction'] # for v2 the json is in this structure 
            edit_category = image_meta_data_sample['category'] # getting the edit category from one of the original data categories 

            lpips_kontext_edit = image_meta_data_sample['lpips_kontext_edit']
            lpips_edit_inversion = image_meta_data_sample['lpips_edit_inversion']
            lpips_inversion_edit = image_meta_data_sample['lpips_inversion_edit']
            lpips_sequence = image_meta_data_sample['lpips_sequence']
            lpips_triangle_sequence = image_meta_data_sample['lpips_sequence_triangle']

            is_sample_valid = False
            if self.filter == "no-filter":
                is_sample_valid = True
            elif self.filter == "simple-filter":
                # filtering for the dataset v2 during training with modified new categories
                is_sample_valid = self.check_sample_validity_v2(
                    lpips_kontext_edit, lpips_edit_inversion, lpips_inversion_edit, edit_category
                )
            elif self.filter == "kl-filter-simple":
                # filtering the samples if the sequence is not smooth enough as the kl divergence between the sequence and the uniform is quite high
                is_sample_valid_edit = self.check_sample_validity_v2(
                    lpips_kontext_edit, lpips_edit_inversion, lpips_inversion_edit, edit_category
                )
                is_sample_smooth = self.check_sample_validity_smoothness(lpips_sequence)
                # combining the two filters to get a final validity flag for the image sample
                is_sample_valid = is_sample_valid_edit and is_sample_smooth
            elif self.filter == "second-order-lpips-simple":
                # checking if the inversion is good in terms of the quality and the inversion of the source and the edit are not that too different 
                is_sample_valid_edit = self.check_sample_validity_v2(
                    lpips_kontext_edit, lpips_edit_inversion, lpips_inversion_edit, edit_category
                )
                # computing the second order lpips smoothness score for the sequence to check for local smoothenss 
                is_sample_smooth_second_order = self.check_sample_validity_second_order_lpips(lpips_triangle_sequence)
                # combinging the two filtering mechanisms to get a single flag based on the inversion quality and smoothness of the edit sequence 
                is_sample_valid = is_sample_valid_edit and is_sample_smooth_second_order
            else:
                raise ValueError(f"Unknown filter type: {self.filter}")
            
            if is_sample_valid:
                filtered_meta_data.append(image_meta_data_sample)

        return filtered_meta_data

    # This function will disintegrate the image into a list of images, the stack has images horizontally stacked with each other and we want to crop the images 
    def disintegrate_image_stack(self, image_stack):
        width, height = image_stack.size
        n_imgs = width // height
        img_width = width // n_imgs
        img_height = height 

        list_stacked_images = []
        # Iterating over the number of images for the edits and saving cropped images in the list 
        for i in range(n_imgs):
            list_stacked_images.append(image_stack.crop((img_width * i, 0, img_width * (i + 1), img_height)))
        
        return list_stacked_images 

    # This function will sample a random image from the interpolation and will return the source image and the interpolated image along with its index, 
    # The index will range from 1 to n_edits - 1, as [1-6] in our case  
    def sample_from_list(self, image_list, n_edits):
        # Ignore any global seed for this block to ensure true randomness
        state = random.getstate()
        random.seed(np.random.randint(0, 1000000)) 
        sampled_idx = random.randint(0, n_edits-1) # this will sample all the way from 0 to 6, in total 7 edits, we are sampling the zeroth edit as well for regularization of training. 
        random.setstate(state)

        # print("sampled_idx: {}".format(sampled_idx))    

        src_img = image_list[0] # taking the 
        edit_img = image_list[sampled_idx] # can also have the zeroth edit as we are including that into training 
        # getting the slider value as the fraction value between [1/6 to 1.0] that will be used for training 
        slider_value = sampled_idx / (n_edits - 1) # dividing by 6, so the edits are in a range of (0,1] 
        return src_img, edit_img, slider_value 
    
    # this function will compute the lpips between any two given images and returns the distance between them. 
    def compute_lpips(self, image_1, image_2):
        image1 = self.to_tensor(image_1)
        image2 = self.to_tensor(image_2)
        distance_lpips = self.loss_fn_vgg(image1, image2)
        return distance_lpips
    
    # This function will extract a random image from the stack and the corresponding slider value will be used to generate the edit and the output 
    def sample_edit_pair(self, image_stack, n_edits): 
        list_stacked_images = self.disintegrate_image_stack(image_stack)
        # print("list_stacked_images: {}".format(len(list_stacked_images)))  

        # for testing purposes, saving the intermediate images and analyzing is the data being accessed correctly 
        # for idx in range(len(list_stacked_images)):
        #     list_stacked_images[idx].save(f"./debug/image_{idx}.png") 

        # the first and the last images are the real images and the intermediate ones as the interpolated images 
        list_interpolated_images = list_stacked_images[1:-1]

        # this function will sample a random image and a scalar value according to it between [1/(n_edits-1) to 1.0] as the extent of the edits   
        src_img, edit_img, slider_value = self.sample_from_list(list_interpolated_images, n_edits)  

        # returning the extracted edit image and the edit strength that is normalized between [0.0 to 1.0] 
        return src_img, edit_img, slider_value, list_stacked_images


    # This function will help us to compute lpips between the images in the stack and that will be used for filtering the data based on the given thresholds.
    def evaluate_lpips_scores(self, list_images):                # Computing the lpips distance between the images from the stack that will be used for filtering 
        score_kontext_edit = self.compute_lpips(list_images[-1], list_images[0]) # taking the first and the last image for checking if the flux has done the correct editing or not 
        score_edit_inversion = self.compute_lpips(list_images[-1], list_images[-2]) # taking the first and the second last image for checking the quality of flux inversion 
        score_inversion_edit = self.compute_lpips(list_images[-2], list_images[1]) # taking the second and the second last from the sequence for checking the edit sequence and its quality 

        return score_kontext_edit.item(), score_edit_inversion.item(), score_inversion_edit.item() 

    # This function will check if the sample is valid based on the lpips scores, numbers are tuned for dataset v1, this will depend on the category of the edit 
    def check_sample_validity_v1(self, lpips_kontext_edit, lpips_edit_inversion, lpips_inversion_edit, category):
        is_valid = True
        if category == 'stylization':
            # this is the check if the strength of the flux edit is too weak for stylization, then we can remove that sample assuming the edit has not worked 
            # there are some apperance edits as well here so keeping in mind that those edits change only small image regions, hence a lower threshold for the appearance. 
            if lpips_kontext_edit < 0.10: 
                is_valid = False 
            # using a higher threshold as in most of the cases the inversion is good below 0.50 threshold and the edit is consistent. 
            if lpips_edit_inversion > 0.50: # this is the check if the inverted image is too different for the edited image, in that case we can remove the sample. 
                is_valid = False 
        elif category == 'appearance':
            # for appearance a smaller threshold between the original and edited image as sometimes the objects can be smaller leading to a small threshold
            if lpips_kontext_edit < 0.065:   
                is_valid = False 
            if lpips_edit_inversion > 0.50: # this can be same for all the categories of edits. 
                is_valid = False 
        elif category == 'reimagining':
            # this is the largest threshold as most of the reimaging images have significant changes in the input images during the edit stage. 
            if lpips_kontext_edit < 0.20: 
                is_valid = False
            if lpips_edit_inversion > 0.50: # this is the same for all the categories as the inversion should be good to consider the sequence as appropriate. 
                is_valid = False 
        
        return is_valid  

    # this function will check the sample validity for the dataset v2 currently with new set of categories 
    def check_sample_validity_v2(self, lpips_kontext_edit, lpips_edit_inversion, lpips_inversion_edit, category):
        # categories = ['stylization', 'environment_change', 'reimagining', 'appearance change', 'attribute change', 'material change', 'object morphing'] 
        is_valid = True
        if category == 'stylization' or category == 'environment_change': # this is good already 
            # this is the check if the strength of the flux edit is too weak for stylization, then we can remove that sample assuming the edit has not worked 
            # there are some apperance edits as well here so keeping in mind that those edits change only small image regions, hence a lower threshold for the appearance. 
            if lpips_kontext_edit < 0.10: 
                is_valid = False 
            # using a higher threshold as in most of the cases the inversion is good below 0.50 threshold and the edit is consistent. 
            if lpips_edit_inversion > 0.55: # this is the check if the inverted image is too different for the edited image, in that case we can remove the sample. 
                is_valid = False 
        elif category == 'reimagining': # this is good already 
            # this is the largest threshold as most of the reimaging images have significant changes in the input images during the edit stage. 
            if lpips_kontext_edit < 0.10: 
                is_valid = False
            if lpips_edit_inversion > 0.52: # this is the same for all the categories as the inversion should be good to consider the sequence as appropriate. 
                is_valid = False 
        elif category == 'appearance_change' or category == 'material_change': # find new values for these categories 
            # for appearance a smaller threshold between the original and edited image as sometimes the objects can be smaller leading to a small threshold
            if lpips_kontext_edit < 0.020: # sometimes only the attribute is changed which is very small in size    
                is_valid = False 
            if lpips_edit_inversion > 0.50: # this can be same for all the categories of edits. 
                is_valid = False 
        elif category == 'attribute_change' or category == 'object_morphing': # find new values for these categories  
            # this is the largest threshold as most of the reimaging images have significant changes in the input images during the edit stage. 
            if lpips_kontext_edit < 0.030: # there are minor changes that need to be captured here hence, not filtering heavily  
                is_valid = False
            if lpips_edit_inversion > 0.45: # this is the same for all the categories as the inversion should be good to consider the sequence as appropriate. 
                is_valid = False 
        
        return is_valid  

    # computes the kl divergence with the uniform sequence 
    def get_kl_divergence(self, unif_dist, norm_lpips_sequence):
        # print("unif_dist: {}".format(unif_dist))
        # print("norm_lpips_sequence: {}".format(norm_lpips_sequence))
        kl = np.sum([pi * np.log(pi/ui) for pi, ui in zip(norm_lpips_sequence, unif_dist) if pi > 0])
        return kl

    # this function will reject the sample if the kl divergence is too high, indicating the sequence is not smooth enough 
    def check_sample_validity_smoothness(self, lpips_sequence):
        unif_dist = [1.0 / len(lpips_sequence) for _ in lpips_sequence]
        norm_lpips_sequence = [v / sum(lpips_sequence) for v in lpips_sequence] 
        kl_divergence = self.get_kl_divergence(unif_dist, norm_lpips_sequence)
        
        if kl_divergence > self.kl_threshold:
            is_valid = False
        else:
            is_valid = True

        return is_valid

    # this function will compute the cdf of the lpips sequence and then returns the mapping to the normalized slider values based on the cdf 
    def get_normalized_sliders(self, lpips_sequence):
        # removing the first and the last images as they are the real images and we are not interested in them 
        lpips_sequence_intermediate = lpips_sequence[1:-1]
        # computing the cummulative sum till that point in the sequence to get the cummulative distribution 
        cumm_lpips = [0] # adding a zero as the first element and the next ones will be subsequent 
        for i in range(len(lpips_sequence_intermediate)):
            cumm_lpips.append(np.sum(lpips_sequence_intermediate[:i+1]))
        # this should be of size 6 + 1 = 7, six edits and one for the source image 

        # normalizing the cummulative sum to get the fraction of change for each of the step in the sequence 
        for i in range(len(cumm_lpips)):
            cumm_lpips[i] = cumm_lpips[i] / cumm_lpips[-1] 

        return cumm_lpips

    # this function will map the fixed slider value to the normalized one based on the cdf of the sequence 
    def get_mapped_slider(self, sliders_cdf, slider_value):
        # here slider values will range from [0, 1/6 .... 6/6] | 7 values in total 
        slider_value_index = int(slider_value * 6)# now the range is from 0-6 
        # print("slider value index: {}".format(slider_value_index))
        slider_value_mapped = sliders_cdf[slider_value_index]  # mapping the sliders 
        return slider_value_mapped


    # Loading a single data point from the dataset 
    def __getitem__(self, idx):
        # Loading the image 
        try:
            # print("requested index: {}".format(idx)) 
            image_meta_data_sample = self.meta_data[idx] # Extracting one row from the meta data, we will use that to get the image name and the edit instruction used for training 
            # print("image_meta_data_sample: {}".format(image_meta_data_sample)) 
            
            # Extacting the image name and the edit instruction from the meta data to perform the processing 
            image_name = image_meta_data_sample['image_name'] # getting the image name 
            edit_instruction = image_meta_data_sample['edit_instruction'] # for v2 the json is in this structure 
            edit_category = image_meta_data_sample['category'] # getting the edit category from one of the original data categories 

            # currently we are interpolating from the dataset that have 7 images stacked togather and there is a starting and the ending image that are the real images in the dataset 
            n_edits = 7 
            image_stack_path = os.path.join(self.imgs_dir, image_name[:-4] + '_nsamples_'+str(n_edits)+'.png') # currently our dataset has 7 intermediate images for the interpolation and hence, it is used in the dataloader 
            img_stack = Image.open(image_stack_path) 
                        
            # loading the lpips scores directly from the corresponding meta data for faster processing
            lpips_sequence = image_meta_data_sample['lpips_sequence']

            sliders_cdf = self.get_normalized_sliders(lpips_sequence)

            # This function will extract the image pairs and the scalar strength values to be used for training the model 
            # returning the stack of images also for computationg of the lpips scores 
            img_src, img_edit, slider_value, list_images_stack = self.sample_edit_pair(img_stack, n_edits)  

            # this function will just map the fixed slider value to the normalized one based on the cdf of the sequence 
            slider_value_mapped = self.get_mapped_slider(sliders_cdf, slider_value) 
            # print("mapped slider value: {}".format(slider_value_mapped))

            # resizing the image to the required size for the source and the edited image for training 
            img_src = img_src.resize((self.img_size, self.img_size)).convert("RGB")
            img_edit = img_edit.resize((self.img_size, self.img_size)).convert("RGB")

                        # Randomly drop text or image
            drop_text = random.random() < self.drop_text_prob
            drop_slider = random.random() < self.drop_slider_prob
            
            if drop_text:
                edit_instruction = ""
            if drop_slider:
                slider_value = 0.5 

            return {
                "src_image": self.image_transforms(img_src),
                "edit_image": self.image_transforms(img_edit),
                "instruction": edit_instruction,
                "slider_value": slider_value, 
                "slider_value_mapped": slider_value_mapped
            }

        except Exception as e:
            # print("Error in the data point: {}".format(e))
            print("moving to the next sample ... ")
            # This will be called if the data point is not valid then, we will access a randomg point from the dataset. 
            return self.__getitem__(np.random.randint(0, self.__len__())) 


# --------------------------------------------------------------------- # 